# GPT-transformer

This project explores Transformer architectures, inspired by the [Attention is All You Need (Vaswani et al., 2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper.

## Overview
Transformers have revolutionized NLP by replacing recurrent and convolutional layers with self-attention mechanisms.  
This repository contains experiments and code implementations related to **GPT (Generative Pre-trained Transformer)** models.

## Example: Tokenization with GPT-2
this is a simple Python code using Hugging Faceâ€™s `transformers` library.

